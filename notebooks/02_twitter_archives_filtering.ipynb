{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b610201-468b-4f47-b43c-c5df0a5ddcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "import bz2\n",
    "import json\n",
    "\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from greenspectors.util.io import download_file, parse_size_to_mb\n",
    "from greenspectors.env import COMPANY_NAMES, DATA_PATH\n",
    "from greenspectors.scraping.twitter_archive import TwitterArchiveSpider, TwitterSingleArchiveSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa3447fa-b111-41f3-a422-4dd0581528f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = get_project_settings()\n",
    "settings['CONCURRENT_REQUESTS'] = 100\n",
    "settings['CONCURRENT_REQUESTS_PER_DOMAIN'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "534ba820-6193-4379-b7ce-011a641b5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 02:50:33 [scrapy.utils.log] INFO: Scrapy 2.5.1 started (bot: scrapybot)\n",
      "2021-10-16 02:50:33 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.5, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.12 (default, Oct 12 2021, 03:01:40) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 21.0.0 (OpenSSL 1.1.1l  24 Aug 2021), cryptography 35.0.0, Platform Windows-10-10.0.19042-SP0\n",
      "2021-10-16 02:50:33 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.selectreactor.SelectReactor\n"
     ]
    }
   ],
   "source": [
    "process = CrawlerProcess(settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d73ad51-95ad-416c-9e57-9e5181ec8441",
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a938490-c227-466a-8592-5a4e1da74651",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "104fd983-86af-43ab-954f-84ab26830190",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterArchiveSpider(scrapy.Spider):\n",
    "    def __init__(self):\n",
    "        self._collected_tweets = []\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://archive.org/details/twitterstream?&sort=-week&page=1\",\n",
    "            \"https://archive.org/details/twitterstream?&sort=-week&page=2\"\n",
    "            # \"https://archive.org/download/archiveteam-twitter-stream-2019-02\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        archive_links = response.css(\"div.results div.item-ia div.C234 a::attr(href)\").getall()\n",
    "        archive_names = [Path(archive_link).stem for archive_link in archive_links]\n",
    "        \n",
    "        for archive_name in archive_names:\n",
    "            yield scrapy.Request(f\"https://archive.org/download/{archive_name}\", callback=self.parse_download_page)\n",
    "        \n",
    "    def parse_download_page(self, response):\n",
    "        sizes = response.css('table.directory-listing-table tr td:last-child::text').getall()\n",
    "        sizes = [parse_size_to_mb(size) for size in sizes]\n",
    "        \n",
    "        file_links = response.css('table.directory-listing-table tr td:first-child a:first-child::attr(href)').getall()[1:]  # Ignore first link, as it just points back to parent directory\n",
    "        file_urls = [f\"{response.url}/{file}/\" for file, size in zip(file_links, sizes) if 100 < size < 5000]\n",
    "        \n",
    "        archive_urls.extend(file_urls)\n",
    "        for file_url in file_urls:\n",
    "            yield scrapy.Request(file_url, callback=self.parse_archive_listing)\n",
    "            break\n",
    "\n",
    "        \n",
    "    def parse_archive_listing(self, response):\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afea2bab-6508-4160-a6e2-cec99e89f1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-16 02:50:33 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'CONCURRENT_REQUESTS': 100, 'CONCURRENT_REQUESTS_PER_DOMAIN': 100}\n",
      "2021-10-16 02:50:33 [scrapy.extensions.telnet] INFO: Telnet Password: 9716c7e6df429fb1\n",
      "2021-10-16 02:50:33 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2021-10-16 02:50:33 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2021-10-16 02:50:33 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2021-10-16 02:50:33 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2021-10-16 02:50:33 [scrapy.core.engine] INFO: Spider opened\n",
      "2021-10-16 02:50:33 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2021-10-16 02:50:33 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2021-10-16 02:50:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://archive.org/download/archiveteam-twitter-stream-2018-06> (referer: None)\n",
      "2021-10-16 02:50:35 [scrapy.downloadermiddlewares.redirect] DEBUG: Redirecting (302) to <GET https://ia800704.us.archive.org/view_archive.php?archive=/29/items/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar> from <GET https://archive.org/download/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar/>\n",
      "2021-10-16 02:50:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://ia800704.us.archive.org/view_archive.php?archive=/29/items/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar> (referer: https://archive.org/download/archiveteam-twitter-stream-2018-06)\n",
      "2021-10-16 02:50:36 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): archive.org:443\n",
      "2021-10-16 02:50:37 [urllib3.connectionpool] DEBUG: https://archive.org:443 \"GET /download/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar/2018%2F06%2F06%2F10%2F44.json.bz2 HTTP/1.1\" 302 None\n",
      "2021-10-16 02:50:37 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ia600704.us.archive.org:443\n",
      "2021-10-16 02:50:38 [urllib3.connectionpool] DEBUG: https://ia600704.us.archive.org:443 \"GET /view_archive.php?archive=/29/items/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar&file=2018%2F06%2F06%2F10%2F44.json.bz2 HTTP/1.1\" 200 None\n",
      "2021-10-16 02:50:42 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2021-10-16 02:50:42 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 1299,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 67640,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/302': 1,\n",
      " 'elapsed_time_seconds': 8.361489,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2021, 10, 16, 0, 50, 42, 116213),\n",
      " 'httpcompression/response_bytes': 370913,\n",
      " 'httpcompression/response_count': 2,\n",
      " 'log_count/DEBUG': 7,\n",
      " 'log_count/INFO': 10,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 3,\n",
      " 'scheduler/dequeued/memory': 3,\n",
      " 'scheduler/enqueued': 3,\n",
      " 'scheduler/enqueued/memory': 3,\n",
      " 'start_time': datetime.datetime(2021, 10, 16, 0, 50, 33, 754724)}\n",
      "2021-10-16 02:50:42 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found samples: 1\n",
      "Found samples: 2\n",
      "Found samples: 3\n",
      "Found samples: 4\n",
      "Found samples: 5\n",
      "Found samples: 6\n",
      "Found samples: 7\n",
      "Found samples: 8\n",
      "Found samples: 9\n",
      "Found samples: 10\n",
      "Found samples: 11\n",
      "Found samples: 12\n",
      "Found samples: 13\n",
      "Found samples: 14\n",
      "Found samples: 15\n",
      "Found samples: 16\n",
      "Found samples: 17\n",
      "Found samples: 18\n",
      "Found samples: 19\n",
      "Found samples: 20\n",
      "Found samples: 21\n",
      "Found samples: 22\n",
      "Found samples: 23\n",
      "Found samples: 24\n",
      "Found samples: 25\n",
      "Found samples: 26\n",
      "Found samples: 27\n",
      "Found samples: 28\n",
      "Found samples: 29\n",
      "Found samples: 30\n",
      "Found samples: 31\n",
      "Found samples: 32\n"
     ]
    }
   ],
   "source": [
    "process.crawl(TwitterSingleArchiveSpider, archive_name='archiveteam-twitter-stream-2018-06')\n",
    "process.start() # the script will block here until the crawling is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6f09e73-c18f-4199-9434-9b9d85fa9f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_urls = []\n",
    "for response in responses:\n",
    "    single_json_urls = response.css('table.archext tr td a::attr(href)').getall()\n",
    "    single_json_urls = [f\"https:{json_url}\" for json_url in single_json_urls]\n",
    "    json_urls.extend(single_json_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0e5bb28-55d3-4df4-bbf9-04afa0fe7793",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(TMP_DATA_FOLDER).mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "78038422-c816-4fb8-8681-2554777ca28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-15 23:27:39 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): archive.org:443\n",
      "2021-10-15 23:27:39 [urllib3.connectionpool] DEBUG: https://archive.org:443 \"GET /download/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar/2018%2F06%2F06%2F10%2F44.json.bz2 HTTP/1.1\" 302 None\n",
      "2021-10-15 23:27:39 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): ia800704.us.archive.org:443\n",
      "2021-10-15 23:27:40 [urllib3.connectionpool] DEBUG: https://ia800704.us.archive.org:443 \"GET /view_archive.php?archive=/29/items/archiveteam-twitter-stream-2018-06/twitter-2018-06-06.tar&file=2018%2F06%2F06%2F10%2F44.json.bz2 HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "url = json_urls[0]\n",
    "file_name = f\"{TMP_DATA_FOLDER}/{'_'.join(Path(url).parts[3:])}\"\n",
    "download_file(url, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd31a781-824f-4bd0-a22d-66fb80cf2a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(file_name, 'r') as f:\n",
    "    asdf  = f.readlines()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6d5fab76-a14c-46e6-9028-45ed4a60a2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@LuuStessens  https://t.co/JlFh74cnME'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(asdf)['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a804901-4a78-4a96-b167-4b09a5acfa5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".tar files: 854\n",
      ".zip files: 478\n"
     ]
    }
   ],
   "source": [
    "suffixes = [Path(url).suffix for url in archive_urls]\n",
    "print(f\".tar files: {len([suffix for suffix in suffixes if suffix == '.tar'])}\")\n",
    "print(f\".zip files: {len([suffix for suffix in suffixes if suffix == '.zip'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "412b6823-893e-4e1d-9830-f083e62500c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = responses[0].css('table.directory-listing-table tr td:last-child::text').getall()\n",
    "sizes = [parse_size_to_mb(size) for size in sizes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c4201834-0059-4aac-a129-26ebdbe798e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_links = responses[0].css('table.directory-listing-table tr td:first-child a:first-child::attr(href)').getall()[1:]  # Ignore first link, as it just points back to parent directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b4393bcb-c251-4032-b7db-b54ac7258282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('twitter_stream_2019_02_01.tar', 607.4),\n",
       " ('twitter_stream_2019_02_02.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_03.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_04.tar', 1600.0),\n",
       " ('twitter_stream_2019_02_05.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_06.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_07.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_08.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_09.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_10.tar', 2000.0),\n",
       " ('twitter_stream_2019_02_11.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_12.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_13.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_14.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_15.tar', 1700.0),\n",
       " ('twitter_stream_2019_02_16.tar', 1700.0),\n",
       " ('twitter_stream_2019_02_17.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_18.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_19.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_20.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_21.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_22.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_23.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_24.tar', 1900.0),\n",
       " ('twitter_stream_2019_02_25.tar', 1800.0),\n",
       " ('twitter_stream_2019_02_26.tar', 1900.0),\n",
       " ('twitter_stream_2019_02_27.tar', 1900.0),\n",
       " ('twitter_stream_2019_02_28.tar', 1800.0)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(file, size) for file, size in zip(file_links, sizes) if 100 < size < 5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bdcd057b-18a2-41f6-b6fb-662e29ed21e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = scrapy.Request(url=url, callback=parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e266d442-709c-4000-a6b2-fd3523e75a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'scrapy.http.request' from 'p:\\\\programming\\\\miniconda\\\\envs\\\\greenspectors\\\\lib\\\\site-packages\\\\scrapy\\\\http\\\\request\\\\__init__.py'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrapy.cr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
